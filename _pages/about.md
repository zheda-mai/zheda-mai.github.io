---
permalink: /
title: About Me
author_profile: true
---

I obtained my MASc. from the [University of Toronto](https://www.utoronto.ca/) advised by [Prof. Scott Sanner](https://d3m.mie.utoronto.ca/members/ssanner/). I mostly worked on Continual Learning and Recommender Systems during my master collaborating with [LG AI Research](https://www.lgresearch.ai/).

Prior to that, I  completed my BASc. in [Engineering Science](https://engsci.utoronto.ca/) at the University of Toronto, where I was fortunate to work with [Dr. Erkang Zhu](http://ekzhu.com/).

I am currently a data scientist at [Optimy.ai](Optimy.ai). Before that, I interned at [Pitney Bowes](https://www.pitneybowes.com/us) as a machine learning engineer in 2019. During my bachelor, I spent a year at [AMD](https://www.amd.com/en) as a software engineer intern. 



# News

- <span style="color:#e67300">March 2022 — New Preprint on arXiv</span>

  [TransCAM: Transformer Attention-based CAM Refinement for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2203.07239) was on arXiv!  We propose TransCAM, a Conformer-based solution to WSSS that explicitly leverages the attention weights from the transformer branch of the Conformer to refine the CAM generated from the CNN branch. TransCAM is motivated by our observation that attention weights from shallow transformer blocks are able to capture low-level spatial feature similarities while attention weights from deep transformer blocks capture high-level semantic context.
  
  
  
- <span style="color:#e67300">Jan 2022 — **WWW 2022**  Acceptance</span>

  [Distributional Contrastive Embedding for Clarification-based Conversational Critiquing](https://ssanner.github.io/papers/www22_dcevae.pdf) was accepted to International World Wide Web Conference (WWW) 2022! In this paper, we propose a novel clarification-based conversational critiquing framework that allows the system to clarify user preferences by using distributional embeddings that can capture the specificity and generality of concepts through distributional coverage. 
  
  
  
- <span style="color:#e67300">Jan 2022 — New Preprint on arXiv</span>

  [Unintended Bias in Language Model-driven Conversational Recommendation](https://arxiv.org/abs/2201.06224) was on arXiv!  We investigate how unintended bias — i.e., language variations such as name references or indirect indicators of sexual orientation or location that should not affect recommendations — manifests in significantly shifted price and category distributions of restaurant recommendations
  
  
  
- <span style="color:#e67300">Nov 2021 — **Artificial Intelligence** Journal Acceptance</span>

  [CVPR 2020 continual learning in computer vision competition: Approaches, results, current challenges and future directions](https://www.sciencedirect.com/science/article/abs/pii/S0004370221001867?dgcid=author) was accepted to Artificial Intelligence!  In this paper, we report the main results of the CVPR 2020 Continual Learning in Computer Vision competition and summarize the winning approaches, current challenges and future research directions.
  
  
  
- <span style="color:#e67300">Oct 2021 — **Neurocomputing** Journal Acceptance</span>

  [Online Continual Learning in Image Classification: An Empirical Survey](https://www.sciencedirect.com/science/article/abs/pii/S0925231221014995) was accepted to Neurocomputing! We empirically scrutinize recently proposed methods and tricks in Online Continual Learning to study their relative advantages and the settings where they work best. We also discuss recent trends and emerging directions in Online Continual Learning. 
  
  
  
- <span style="color:#e67300">April 2021 — **CVPR 2021** Workshop Acceptance</span> 

  Our paper [Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning](https://arxiv.org/abs/2103.13885) was accepted to the [Workshop on Continual Learning in Computer Vision](https://sites.google.com/view/clvision2021/) at **CVPR 2021**! We leverage supervised contrastive learning and nearest class mean classifier to obtain new state-of-the-art performance for online continual learning. 
  
  

- <span style="color:#e67300">Dec 2020 — **AAAI 2021** Acceptance</span>

  Our paper [Online Class-Incremental Continual Learning with Adversarial Shapley Value](http://128.84.4.34/abs/2009.00093) was accepted to **AAAI 2021**! We contribute a novel Adversarial Shapley value scoring method that scores memory data samples according to their ability to preserve latent decision boundaries for previously observed classes (to maintain learning stability and avoid forgetting) while interfering with latent decision boundaries of current classes being learned (to encourage plasticity and optimal learning of new class boundaries). 

  

- <span style="color:#e67300">Nov 2020 — **ICDM 2020** Workshop Acceptance</span>

  Our paper [Attentive Autoencoders for Multifaceted Preference Learning in One-class Collaborative Filtering](https://arxiv.org/abs/2010.12803) (with [Ga Wu](https://wuga214.github.io/), [Kai Luo](https://scholar.google.com/citations?user=lO1PU44AAAAJ&hl=en), [Scott Sanner](https://d3m.mie.utoronto.ca/members/ssanner/)) was aceepted to the [Workshop on Advanced Neural Algorithms and Theories for Recommender Systems (NeuRec)](https://datasj.github.io/) at **ICDM 2020**!

  

- <span style="color:#e67300">June 2020 — **CVPR 2020** CLVision Challenge Champion</span>

  I **won 1st  place** in the **CVPR 2020** [CLVision Challenge](https://sites.google.com/view/clvision2020/challenge/challenge-winners) with my entry [Batch-level Experience Replay with Review for Continual Learning](https://arxiv.org/abs/2007.05683)! Welcome to check our winning solution [[code]](https://github.com/RaptorMai/CVPR20_CLVision_challenge) [[paper]](https://arxiv.org/abs/2007.05683) and the [summary](https://arxiv.org/abs/2009.09929) of the challenge.

  

# Contact

Email: zheda.mai@mail.utoronto.ca

