---
permalink: /
title: About Me
author_profile: true
---
I am a CS Ph.D. student at <ins>The Ohio State University</ins> (Advisor:  [Wei-Lun (Harry) Chao](https://sites.google.com/view/wei-lun-harry-chao)). I obtained my MASc. at <ins>University of Toronto</ins> (Advisor: [Scott Sanner](https://d3m.mie.utoronto.ca/members/ssanner/)), working on Continual Learning collaborating with [LG AI Research](https://www.lgresearch.ai/). I completed BASc. in [Engineering Science](https://engsci.utoronto.ca/) at <ins>University of Toronto</ins>.


My research interests lie in 

- **Efficient, Robust and Interpretable Foundation Model Adaptation**: [NeurIPS'25](https://arxiv.org/abs/2503.09707), [CVPR'25 (<span style="color:red">**Highlight**</span>)](https://openaccess.thecvf.com/content/CVPR2025/html/Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning_CVPR_2025_paper.html), [CVPR'25](https://openaccess.thecvf.com/content/CVPR2025/html/Chowdhury_Prompt-CAM_Making_Vision_Transformers_Interpretable_for_Fine-Grained_Analysis_CVPR_2025_paper.html), [CVPR'25](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Finer-CAM_Spotting_the_Difference_Reveals_Finer_Details_for_Visual_Explanation_CVPR_2025_paper.html), [NeurIPS'24](https://proceedings.neurips.cc/paper_files/paper/2024/hash/f573c36434796efe066d2f4cf3349e7f-Abstract-Conference.html), [CVPR'23](https://openaccess.thecvf.com/content/CVPR2023/html/Tu_Visual_Query_Tuning_Towards_Effective_Usage_of_Intermediate_Representations_for_CVPR_2023_paper.html),       [NeurIPS'23](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5d087955ee13fe9a7402eedec879b9c3-Abstract-Conference.html)
- **Vision Foundation Models & Multimodal Models**: [NeurIPS'25](https://arxiv.org/abs/2505.23883), [NAACL'25](https://arxiv.org/abs/2502.17599), [CVPR'25-W](https://openaccess.thecvf.com/content/CVPR2025W/eLVM/html/Xie_Efficiently_Mitigating_Video_Content_Misalignment_on_Large_Vision_Model_with_CVPRW_2025_paper.html), [NeurIPS'24](https://proceedings.neurips.cc/paper_files/paper/2024/hash/32923dff09f75cf1974c145764a523e2-Abstract-Datasets_and_Benchmarks_Track.html), [NeurIPS'23-W](https://arxiv.org/abs/2305.05803)
- **Continual Learning**: [ICML'25-W](https://openreview.net/forum?id=2FKAoAmYOj), [AAAI'21(<span style="color:red">**Oral**</span>))](https://ojs.aaai.org/index.php/AAAI/article/view/17159), [CVPR'21-W](https://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Mai_Supervised_Contrastive_Replay_Revisiting_the_Nearest_Class_Mean_Classifier_in_CVPRW_2021_paper.html), [Neurocomputing](https://www.sciencedirect.com/science/article/abs/pii/S0925231221014995), [CVPR'20-CompetitionüèÖ](https://arxiv.org/abs/2007.05683), [AIJ](https://www.sciencedirect.com/science/article/abs/pii/S0004370221001867)


<!--[Preprint-AVABench](https://arxiv.org/abs/2506.09082), [Preprint-BioCLIP2](https://arxiv.org/abs/2505.23883),-->


<span style="color:green">**I am actively looking for a research internship!  If you are aware of any opportunities or have any recommendations, I would greatly appreciate your insights and referrals. Please feel free to reach out!**</span>

# News

 - <span style="color:#e67300">Sept 2025 ‚Äî **Two NeurIPS** 2025 Acceptance</span>

	[Revisiting semi-supervised learning in the era of foundation models](https://arxiv.org/abs/2503.09707). We provide a comprehensive emperical study of SSL with VFM and propose a simple but strong baseline leveraging the diverse predictions from different parameter-efficient fine-tuning (PEFT) methods. 
	
	[Bioclip 2: Emergent properties from scaling hierarchical contrastive learning](https://arxiv.org/abs/2505.23883). We curate TreeOfLife-200M, with 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We train BIOCLIP 2 on TREEOFLIFE-200M and found several emergent properties.

 - <span style="color:#e67300">July 2025 ‚Äî **ICML** 2025 Workshop Acceptance</span>

	[An Empirical Exploration of Continual Unlearning for Image Generation](https://openreview.net/pdf?id=2FKAoAmYOj). We present the first systematic study of continual unlearning in text-to-image generation.


- <span style="color:#e67300">June 2025 ‚Äî New preprint about **Atomic Visual Ability** in **Vision Foundation Models**</span>

	[AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082). We introduce AVA-BENCH, the first benchmark that explicitly disentangles 14 Atomic Visual Abilities (AVAs)‚Äîfoundational skills like localization, depth estimation, and spatial understanding that collectively support complex visual reasoning tasks. By decoupling AVAs and matching train/test distributions within each, AVA-BENCH pinpoints exactly where a VFM excels or falters.

- <span style="color:#e67300">May 2025 ‚Äî Research intern at **Amazon Lab126** </span>

	I will join Amazon Lab126 as a research intern working on multimodal models. 


- <span style="color:#e67300">Feb 2025 ‚Äî **Three CVPR 2025 Main Conference & One Workshop** Acceptance</span>
		
	[Lessons and Insights from a Unifying Study of Parameter-Efficient Fine-Tuning (PEFT) in Visual Recognition. ](https://zheda-mai.github.io/PEFT_Vision_CVPR25/)  was selected as <span style="color:red">**Highlight (2.98%)**</span>. Instead of chasing the leaderboard, we offer a complementary perspective of PEFT by conducting a unifying empirical study. We provide (1) a systematic framework for reproducible evaluations; (2) empirical recommendations on how to use different PEFT in various scenarios (low-shots, many-shots, varying domain gaps, and robustness to distribution shifts); (3) insightful directions for future research.

	[Prompt-CAM: A Simpler Interpretable Transformer for Fine-Grained Analysis. ](https://arxiv.org/pdf/2501.09333). We present PROMPT-CAM, an easily implementable, trainable, and reproducible interpretable method that
leverages the representations of pre-trained ViTs to identify and localize traits for fine-grained analysis.

	[Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation. ](https://arxiv.org/pdf/2501.11309). We propose
Finer-CAM, a method that explicitly compares the target
class with similar classes to reveal the most discriminative
feature channels.


	[Mitigating Video Content Misalignment on Large Vision Model with Time-Series Data Alignment]() was accepted to [Efficient Large Vision Models Workshop](https://sites.google.com/view/elvm/home).  
	
- <span style="color:#e67300">Jan 2025 ‚Äî **NAACL 2025** Acceptance</span>

	[Attention Entropy-Guided Dynamic Cache Allocation for Efficient Multimodal Long-Context Inference](https://2025.naacl.org/) was accepted to **NAACL 2025**. We present a novel approach specifically designed for the complexities of multimodal settings, dynamically allocating KV cache sizes based on attention entropy to better adapt to multimodal interactions.
	
- <span style="color:#e67300">Dec 2024 ‚Äî **ICASSP 2025** Acceptance</span>

	[Attention-Driven Causal Discovery: From Transformer Matrices to Granger Causal Graphs for Non-Stationary Time-series Data](https://2025.ieeeicassp.org/) was accepted to **ICASSP 2025**. We present a two-stage approach for causal discovery in non-stationary multivariate time series data.

- <span style="color:#e67300">Sept 2024 ‚Äî Two **NeurIPS 2024** Acceptances</span> 
	
  [Fine-Tuning is Fine, if Calibrated. ](https://arxiv.org/abs/2409.16223) was accepted to **NeurIPS** 2024. Fine-tuning a pre-trained classifier capable of recognizing a large number of classes to master a subset of classes at hand is shown to drastically degrade the performance in the other classes it had previously learned. We proposed simple post-processing calibration to bring back the pre-trained model‚Äôs capability. 
  
  [MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs. ](https://arxiv.org/pdf/2407.16837) was accepted to **NeurIPS** 2024. We introduce MLLM-COMPBENCH to evaluate the comparative reasoning capability of MLLMs, which contains 40K image pairs with visually oriented questions covering 8 relativities: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality.



- <span style="color:#e67300">May 2024 ‚Äî Research intern at **Bosch** </span>

	I will join Bosch as a research intern working on time series + vision + language with foundation models. 


- <span style="color:#e67300">Oct 2023 ‚Äî **NeurIPS 2023** Outstanding Reviewer </span>

	I am thrilled to be selected as an Outstanding Reviewer for the NeurIPS 2023 conference.

- <span style="color:#e67300">Oct 2023 ‚Äî **NeurIPS 2023** Acceptance </span>

  [Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5d087955ee13fe9a7402eedec879b9c3-Abstract-Conference.html) was accepted to NeurIPS 2023. We address a learning problem involving the adaptation of a pre-trained source model, capable of classifying a wide range of objects to a target domain using
data that covers only a partial label space. 


- <span style="color:#e67300">Oct 2023 ‚Äî **NeurIPS 2023** Workshop Acceptance </span>

  [Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2305.05803) was accepted to NeurIPS2023 [I Can‚Äôt Believe It‚Äôs Not Better (ICBINB): Failure Modes in the Age of Foundation Models](https://sites.google.com/view/icbinb-2023/home) Workshoip. We leverage the Segment Anything Model (SAM) to enhanced pseudo labels for Weakly Supervised Semantic Segmentation (WSSS). 
  
- <span style="color:#e67300">Feb 2023 ‚Äî **CVPR 2023** Acceptance </span>

  [Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning](https://arxiv.org/abs/2212.03220) was accepted to CVPR 2023. We propose visual query tuning (VQT), a simple yet effective approach to aggregate intermediate features of Vision Transformers. 

<h1>Previous News</h1>

<details> 
<summary> Click here to see old news </summary>

<div markdown="1">

* <span style="color:#e67300">Oct 2022 ‚Äî **IPM** Acceptance</span>

  [Unintended Bias in Language Model-driven Conversational Recommendation](https://arxiv.org/abs/2201.06224) was accepted to Information Processing and Management (IPM)!  We investigate how unintended bias ‚Äî i.e., language variations such as name references or indirect indicators of sexual orientation or location that should not affect recommendations ‚Äî manifests in significantly shifted price and category distributions of restaurant recommendations


* <span style="color:#e67300">Sept 2022 ‚Äî **ECCV 2022** Workshop Acceptance</span>

  [TransCAM: Transformer Attention-based CAM Refinement for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2203.07239) was to the [Learning from Limited and Imperfect Data (L2ID) Workshop](https://l2id.github.io/l2id2022/) at ECCV 2022!  We propose TransCAM, a Conformer-based solution to WSSS that explicitly leverages the attention weights from the transformer branch of the Conformer to refine the CAM generated from the CNN branch. TransCAM is motivated by our observation that attention weights from shallow transformer blocks are able to capture low-level spatial feature similarities while attention weights from deep transformer blocks capture high-level semantic context. 


* <span style="color:#e67300">April 2022 ‚Äî **SIGIR 2022**¬† Acceptance</span>

  [Mitigating the Filter Bubble while Maintaining Relevance: Targeted Diversification with VAE-based Recommender Systems](https://sigir.org/sigir2022/) was accepted to ACM SIGIR 2022! In this paper, we propose a novel methodology that trains Concept Activation Vectors (CAVs) for targeted topical dimensions (e.g., political polarization). We then modulate the latent embeddings of user preferences in a state-of-the-art VAE-based recommender system to diversify along the targeted dimension while preserving topical relevance across orthogonal dimensions.
  
  
 
  
  
  
* <span style="color:#e67300">Jan 2022 ‚Äî **WWW 2022**¬† Acceptance</span>

  [Distributional Contrastive Embedding for Clarification-based Conversational Critiquing](https://ssanner.github.io/papers/www22_dcevae.pdf) was accepted to International World Wide Web Conference (WWW) 2022! In this paper, we propose a novel clarification-based conversational critiquing framework that allows the system to clarify user preferences by using distributional embeddings that can capture the specificity and generality of concepts through distributional coverage. 
  
  
  
  
  
* <span style="color:#e67300">Nov 2021 ‚Äî **Artificial Intelligence**¬†Journal Acceptance</span>

  [CVPR 2020 continual learning in computer vision competition: Approaches, results, current challenges and future directions](https://www.sciencedirect.com/science/article/abs/pii/S0004370221001867?dgcid=author) was accepted to Artificial Intelligence!  In this paper, we report the main results of the CVPR 2020 Continual Learning in Computer Vision competition and summarize the winning approaches, current challenges and future research directions.
  
  
  
* <span style="color:#e67300">Oct 2021 ‚Äî **Neurocomputing**¬†Journal Acceptance</span>

  [Online Continual Learning in Image Classification: An Empirical Survey](https://www.sciencedirect.com/science/article/abs/pii/S0925231221014995) was accepted to Neurocomputing! We empirically scrutinize recently proposed methods and tricks in Online Continual Learning to study their relative advantages and the settings where they work best. We also discuss recent trends and emerging directions in Online Continual Learning. 
  
  
  
* <span style="color:#e67300">April 2021 ‚Äî **CVPR 2021**¬†Workshop Acceptance</span> 

  Our paper [Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning](https://arxiv.org/abs/2103.13885) was accepted to the [Workshop on Continual Learning in Computer Vision](https://sites.google.com/view/clvision2021/) at **CVPR 2021**! We leverage supervised contrastive learning and nearest class mean classifier to obtain new state-of-the-art performance for online continual learning. 
  
  

* <span style="color:#e67300">Dec 2020 ‚Äî **AAAI 2021** Acceptance</span>

  Our paper [Online Class-Incremental Continual Learning with Adversarial Shapley Value](http://128.84.4.34/abs/2009.00093) was accepted to **AAAI 2021**! We contribute a novel Adversarial Shapley value scoring method that scores memory data samples according to their ability to preserve latent decision boundaries for previously observed classes (to maintain learning stability and avoid forgetting) while interfering with latent decision boundaries of current classes being learned (to encourage plasticity and optimal learning of new class boundaries). 

  

* <span style="color:#e67300">Nov 2020 ‚Äî **ICDM 2020** Workshop Acceptance</span>

  Our paper [Attentive Autoencoders for Multifaceted Preference Learning in One-class Collaborative Filtering](https://arxiv.org/abs/2010.12803) (with [Ga Wu](https://wuga214.github.io/), [Kai Luo](https://scholar.google.com/citations?user=lO1PU44AAAAJ&hl=en), [Scott Sanner](https://d3m.mie.utoronto.ca/members/ssanner/)) was accepted to the [Workshop on Advanced Neural Algorithms and Theories for Recommender Systems (NeuRec)](https://datasj.github.io/) at **ICDM 2020**!

  

* <span style="color:#e67300">June 2020 ‚Äî **CVPR 2020** CLVision Challenge Champion</span>

  I **won 1st  place** in the **CVPR 2020** [CLVision Challenge](https://sites.google.com/view/clvision2020/challenge/challenge-winners) with my entry [Batch-level Experience Replay with Review for Continual Learning](https://arxiv.org/abs/2007.05683)! Welcome to check our winning solution [[code]](https://github.com/RaptorMai/CVPR20_CLVision_challenge) [[paper]](https://arxiv.org/abs/2007.05683) and the [summary](https://arxiv.org/abs/2009.09929) of the challenge.

</div>
</details>


# Contact

Email: mai.145@osu.edu

