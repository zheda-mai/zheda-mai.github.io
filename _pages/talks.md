---
permalink: /talks/
author_profile: true
---
* [Visual query tuning: Towards effective usage of intermediate representations for parameter and memory efficient transfer learning. ](https://www.youtube.com/watch?v=mjEd3YLcrKM)

 Presentation at **CVPR 2023**, Vancouver Canada

* [Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning](https://www.youtube.com/watch?v=030Hzmg-mow&list=LL&index=3)

  Oral presentation at **CVPR 2021** Workshop on Continual Learning

* [Online Class-Incremental Continual Learning with Adversarial Shapley Value](https://studio.slideslive.com/web_recorder/share/20210109T023929Z__AAAI__9988__online-class-incremental-conti?s=91a296f8-bf46-4355-b46e-d09f907637dd)

  Oral presentation at **AAAI 2021**, Vancouver Canada (Virtual due to COVID-19)

* [Batch-level Experience Replay with Review for Continual Learning](https://youtu.be/AbgbzTDZRq8?t=1283)

  Oral presentation at Workshop on Continual Learning in Computer Vision, **CVPR 2020**, Seattle, United States (Virtual due to COVID-19)

* [Attentive Autoencoders for Multifaceted Preference Learning in One-class Collaborative Filtering](https://screencast-o-matic.com/watch/cY6ZhCs1aZ)

  Oral presentation at Workshop on Advanced Neural Algorithms and Theories for Recommender Systems, **ICDM 2020**, Sorrento Italy (Virtual due to COVID-19)


# Reading group paper presentation
* [Robustness in deep learning](/files/Robustness_in_Deep_Learning.pptx):
	* Adversarial robustness
	* Explicit Inductive Bias for Transfer Learning with Convolutional Networks, ICML 2018
	* Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution, ICML 2022
	* Robust fine-tuning of zero-shot models, CVPR2022
	* Finetune like you pretrain: Improved finetuning of zero-shot vision models, CVPR2023
	* Spurious Features Everywhere -- Large-Scale Detection of Harmful Spurious Features in ImageNet, ICCV2023
	* Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning, ICML 2023
	* Zero-Shot Robustification of Zero-Shot Models, ICLR2024
	* Contrastive Adapters for Foundation Model Group Robustness, NeurIPS2022
	* Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations, ICML2023

* [Recent Meta papers](/files/Meta_papers.pptx):
	* Scalable Diffusion Models with Transformers, ICCV23 
	* Deconstructing Denoising Diffusion Models for Self-Supervised Learning.
	* A Decade’s Battle on Dataset Bias: Are We There Yet? 
* [Mixture of Experts](/files/Mixture-of-Experts.pptx)
    * Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, ICLR2017
    * Scaling Vision with Sparse Mixture of Experts, NeurIPS2021  
    * Learning to Route by Task for Efficient Inference, EMNLP2021
    * Mixture-of-Experts with Expert Choice Routing, NeurIPS2022
    * Multimodal Contrastive Learning with LIMoE: the Language Image Mixture of Experts, NeurIPS2022
    * Towards Understanding the Mixture-of-Experts Layer in Deep Learning, NeurIPS2022
    * Sparse Mixture-of-Experts are Domain Generalizable Learners, ICLR2023
    * M³ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design, NeurIPS2022
    * Task-customized Masked Autoencoder via Mixture of Cluster-conditional Experts, ICLR2023
    * The Power of External Memory in Increasing Predictive Model Capacity
    * Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints, ICLR2023

* [Frustratingly Easy Transferability Estimation](/files/Transferability.pptx)
    * Frustratingly Easy Transferability Estimation, ICML2022


# Talks
* [A Simple and Effective Approach to Continual Learning for Image Classification](/files/vector.pdf) 

  Vector Institute, Toronto Canada, 2020

* [Tutorial of Seq2Seq & Attention](/files/attention.pptx)

  Data-Driven Decision Making Lab (D3M), University of Toronto, 2020



# Thesis

[Online Continual Learning In Image Classification](/files/ZhedaMai_Thesis.pdf)

Committee: [Prof. Elias Khalil](https://ekhalil.com/), [Prof. Scott Sanner](https://d3m.mie.utoronto.ca/members/ssanner/)

